import os
from matplotlib import pyplot as plt
import cv2

def check_filetype(filename):
    """
    D√©termine le type de fichier en fonction de son extension.

    Cette fonction prend un nom de fichier en entr√©e, extrait son extension et d√©termine
    le type de fichier (par exemple, image, vid√©o). Si l'extension du fichier est reconnue comme un format
    d'image courant (jpg, png, tiff, svg) ou un format de vid√©o courant (mp4, avi, mkv), elle attribue
    le type correspondant. Sinon, le type de fichier est d√©fini sur None.

    Param√®tres :
    - filename (str) : Le chemin vers le fichier incluant le nom de fichier.

    Retourne :
    - str ou None : Le type de fichier d√©termin√© ('image', 'vid√©o') ou None si le type de fichier
      n'est pas reconnu.

    Exemple :
    >>> check_filetype("/chemin/vers/image.jpg")
    'image'
    >>> check_filetype("/chemin/vers/video.mp4")
    'vid√©o'
    >>> check_filetype("/chemin/vers/fichierinconnu.xyz")
    None
    """

    # Extrait le nom de base du fichier √† partir du chemin de fichier fourni.
    file_basename = os.path.basename(filename)

    # S√©pare le nom de base sur le point et prend la derni√®re partie comme extension.
    extension = file_basename.split(".")[-1]

    # D√©termine le type de fichier en fonction de l'extension.
    if extension in ["jpg", "png", "tiff", "svg"]:
        filetype = "image"
    elif extension in ["mp4", "avi", "mkv"]:
        filetype = "vid√©o"
    else:
        filetype = None

    # Enregistre le type de fichier d√©tect√©.
    print(f"[INFO] : Le fichier {file_basename} est de type : {filetype}")
    
    return filetype

TEST_VIDEO_FILE = "./assets/tuto_maquillage.mp4"
TEST_IMAGE_FILE = "./assets/selfie_with_johnny-depp.png"

check_filetype(TEST_VIDEO_FILE)
check_filetype(TEST_IMAGE_FILE)

def extract_frame_video(video_path, frame_id):
    """
    Extrait une image sp√©cifique d'une vid√©o.

    Cette fonction utilise OpenCV pour ouvrir une vid√©o √† partir du chemin sp√©cifi√© et extrait une image
    particuli√®re en fonction de son ID. L'ID de l'image correspond √† l'ordre de l'image dans la vid√©o, en commen√ßant
    par 0 pour la premi√®re image. Si l'extraction r√©ussit, l'image est retourn√©e sous forme d'un tableau Numpy.

    Param√®tres :
    - video_path (str) : Le chemin vers le fichier vid√©o d'o√π extraire l'image.
    - frame_id (int) : L'identifiant (ID) de l'image √† extraire.

    Retourne :
    - ndarray ou None : L'image extraite (un tableau Numpy) si l'extraction est r√©ussie,
      sinon `None`.

    Exemple :
    >>> image = extract_frame_video("/chemin/vers/video.mp4", 150)
    >>> type(image)
    <class 'numpy.ndarray'>
    """

    # Ouvre la vid√©o √† partir du chemin fourni.
    video = cv2.VideoCapture(video_path)

    # Positionne le lecteur vid√©o sur l'image sp√©cifi√©e par frame_id.
    video.set(cv2.CAP_PROP_POS_FRAMES, frame_id)

    # Lit l'image actuelle.
    ret, image = video.read()

    # Si la lecture r√©ussit (ret est True), retourne l'image.
    # Sinon, retourne None.
    return image if ret else None



TEST_VIDEO_FILE = "./assets/tuto_jeux-video.mp4"
photo = extract_frame_video(TEST_VIDEO_FILE, 1)

# V√©rifier si l'image a √©t√© extraite avec succ√®s
if photo is not None:
    # Conversion de l'image BGR (OpenCV) en RGB (Matplotlib)
    photo_rgb = cv2.cvtColor(photo, cv2.COLOR_BGR2RGB)

    # Affichage avec Matplotlib
    plt.imshow(photo_rgb)
    plt.axis('off')  # Supprimer les axes pour un affichage propre
    plt.title("Premi√®re image extraite")
    plt.show()
else:
    print("Erreur : Impossible d'extraire l'image.")

TEST_VIDEO_FILE = "./assets/tuto_jeux-video.mp4"
photo = extract_frame_video(TEST_VIDEO_FILE, 99)

# V√©rifier si l'image a √©t√© extraite avec succ√®s
if photo is not None:
    # Conversion de l'image BGR (OpenCV) en RGB (Matplotlib)
    photo_rgb = cv2.cvtColor(photo, cv2.COLOR_BGR2RGB)

    # Affichage avec Matplotlib
    plt.imshow(photo_rgb)
    plt.axis('off')  # Supprimer les axes pour un affichage propre
    plt.title("Premi√®re image extraite")
    plt.show()
else:
    print("Erreur : Impossible d'extraire l'image.")


# <h4 style="text-align: left; color:#20a08d; font-size: 25px"><span><strong> Mod√©ration d'une image
# </strong></span></h4>

# La fonction `get_aws_session` ci-dessous permet de se connecter √† une session AWS en utilisant les cl√©s d'acc√®s et cl√©s secr√®tes.

# In[ ]:


#!pip install boto3 python-dotenv 
#!pip install nltk


# In[ ]:


import os, boto3
from dotenv import load_dotenv

def get_aws_session():
    """
    Cr√©e et retourne une session AWS.

    Cette fonction charge les variables d'environnement depuis un fichier .env situ√© dans le r√©pertoire
    courant ou les parents de celui-ci, r√©cup√®re les cl√©s d'acc√®s AWS (`ACCESS_KEY` et `SECRET_KEY`),
    et initialise une session AWS avec ces identifiants ainsi qu'avec une r√©gion sp√©cifi√©e (dans cet exemple,
    'us-east-1'). Elle est particuli√®rement utile pour configurer une session AWS de mani√®re s√©curis√©e sans
    hardcoder les cl√©s d'acc√®s dans le code.

    Retourne :
    - Session : Un objet session de boto3 configur√© avec les cl√©s d'acc√®s et la r√©gion AWS.

    Exemple d'utilisation :
    >>> session_aws = get_aws_session()
    >>> type(session_aws)
    <class 'boto3.session.Session'>
    """

    # Charge les variables d'environnement depuis .env.
    load_dotenv()

    # Cr√©e une session AWS avec les cl√©s d'acc√®s et la r√©gion d√©finies dans les variables d'environnement.
    aws_session = boto3.Session(
        aws_access_key_id=os.getenv("ACCESS_KEY"),        # R√©cup√®re l'ID de cl√© d'acc√®s depuis les variables d'environnement.
        aws_secret_access_key=os.getenv("SECRET_KEY"),    # R√©cup√®re la cl√© d'acc√®s secr√®te depuis les variables d'environnement.
        region_name="us-east-1"                           # Sp√©cifie la r√©gion AWS √† utiliser.
    )
    
    # Retourne l'objet session cr√©√©.
    return aws_session


# Passons maintenant au d√©veloppement de la fonction `moderate_image`. Cette fonction prendra en entr√©e une image et renverra la liste des th√®mes choquants pr√©sents dans l'image, s'il y'en a. 

# <p style="text-align: left; font-size: 16px; color:#7a0f43"><span>‚ùì Quelle service AWS serait le plus indiqu√© pour r√©aliser ce traitement ?</span></p>

# In[ ]:


# <p style="text-align: left; font-size: 16px; color:#131fcf"><span>üñ•Ô∏è  Ecrivez le code dans la fonction  <strong>moderate_image</strong> permettant d'analyser une image et d√©tecter les sujets de mod√©ration </span></p>

# In[104]:


def moderate_image(image_path, aws_service):
    """
    D√©tecte du contenu n√©cessitant une mod√©ration dans une image en utilisant un service AWS sp√©cifi√©.

    Cette fonction ouvre une image depuis un chemin donn√©, puis utilise le service AWS (comme Amazon Rekognition)
    pour d√©tecter les contenus potentiellement inappropri√©s ou sensibles (comme la nudit√©, la violence, etc.).
    Elle collecte et retourne une liste des √©tiquettes de mod√©ration identifi√©es pour cette image.

    Param√®tres :
    - image_path (str) : Le chemin vers l'image √† analyser.
    - aws_service (object) : Un objet de service AWS configur√©, capable de r√©aliser des op√©rations de d√©tection
      de contenu n√©cessitant une mod√©ration (par exemple, un client Amazon Rekognition).

    Retourne :
    - list[str] : Une liste des noms des √©tiquettes de mod√©ration d√©tect√©es pour l'image.

    Exemple d'utilisation :
    >>> aws_rekognition_client = boto3.client('rekognition', region_name='us-east-1')
    >>> moderate_image("/chemin/vers/image.jpg", aws_rekognition_client)
    ['Nudity', 'Explicit Violence']
    """
    with open(image_path, 'rb') as image_file:
        image_bytes = image_file.read()

    # Appeler le service AWS (ici Rekognition) pour analyser l'image
    response = aws_service.detect_moderation_labels(
        Image={'Bytes': image_bytes}
    )

    # Extraire les √©tiquettes de mod√©ration de la r√©ponse
    moderation_labels = response.get('ModerationLabels', [])
    inappropriate_themes = [
        label['Name'] for label in moderation_labels if label['Confidence'] > 80
    ]

    if(inappropriate_themes):
        return inappropriate_themes
    else:
        return None




# <p style="text-align: left; font-size: 16px; color:#131fcf"><span>üñ•Ô∏è  Ecrivez le code permettant de tester la fonction  <strong>moderate_image</strong>. Pour ce faire : <ul style="text-align: left; font-size: 16px; color:#131fcf">
#     <li>Instancier une session AWS avec vos cl√©s</li>
#     <li>Instancier le service AWS appropri√© pour ce traitement </li>
#     <li>Appelez la fonction <code style="text-align: left; font-size: 16px; color:#131fcf">moderate_image</code> avec ce service comme argument afin de recueillir la liste potentielle des th√®mes choquants</li>
#     </ul> </span></p>

# In[33]:


import os
import boto3
from dotenv import load_dotenv
def get_aws_session():
    """
    Cr√©e et retourne une session AWS.
    Charge les cl√©s d'acc√®s depuis un fichier .env pour une configuration s√©curis√©e.
    """
    # Charger les variables d'environnement √† partir du fichier .env
    load_dotenv()

    # Cr√©er une session AWS avec les cl√©s d'acc√®s et la r√©gion sp√©cifi√©e
    return boto3.Session(
        aws_access_key_id=os.getenv("ACCESS_KEY"),
        aws_secret_access_key=os.getenv("SECRET_KEY"),
        region_name="us-east-1"  # Vous pouvez changer la r√©gion si n√©cessaire
    )

TEST_IMAGE_FILE_1 = "./assets/haine.png"
TEST_IMAGE_FILE_2 = "./assets/vulgaire.png"
TEST_IMAGE_FILE_3 = "./assets/violence1.png"
TEST_IMAGE_FILE_4 = "./assets/no-violence1.png"

def test_moderate_image():
    # Instancier une session AWS
    aws_session = get_aws_session()

    # Cr√©er un client pour Amazon Rekognition
    rekognition_client = aws_session.client('rekognition')

    # Liste des fichiers d'image √† tester
    image_files = [
        "./assets/haine.png",
        "./assets/vulgaire.png",
        "./assets/violence1.png",
        "./assets/no-violence1.png"
    ]

    # Tester chaque image
    for image_file in image_files:
        print(f"Analyse de l'image : {image_file}")
        themes_choquants = moderate_image(image_file, rekognition_client)
        
        # Affichage des r√©sultats pour chaque image
        if themes_choquants:
            print(f"Th√®mes choquants d√©tect√©s dans '{image_file}': {themes_choquants}")
        else:
            print(f"Aucun th√®me choquant d√©tect√© dans '{image_file}'.")
        print("-" * 50)

        
# Lancer le test
test_moderate_image()


# <h4 style="text-align: left; color:#20a08d; font-size: 25px"><span><strong> Production de sous-titres
# </strong></span></h4>

# La production de sous-titres √† partir d'une vid√©o s'appuiera sur la technologie speech-to-text d'AWS.

# <div class="alert alert-info">
#   <strong>BUCKET S3</strong><br><br> Au pr√©alable, assurez-vous d'avoir cr√©√© un bucket S3 puisque la transcription speech-to-text n√©cessite que le fichier transcrit soit d√©pos√© dans un bucket S3
# </div>

# <p style="text-align: left; font-size: 16px; color:#131fcf"><span>üñ•Ô∏è  Ecrivez le code permettant d'instancier un client S3 puis de cr√©er un bucket </span></p>

# In[34]:


import boto3
import os
from dotenv import load_dotenv

def get_aws_session():
    # Charger les variables d'environnement depuis le fichier .env
    load_dotenv()

    # Cr√©er une session AWS avec les cl√©s d'acc√®s et la r√©gion sp√©cifi√©e
    return boto3.Session(
        aws_access_key_id=os.getenv("ACCESS_KEY"),
        aws_secret_access_key=os.getenv("SECRET_KEY"),
    ) 

def create_s3_bucket(bucket_name):
    # Cr√©er une session AWS
    aws_session = get_aws_session()

    # Cr√©er un client S3
    s3_client = aws_session.client("s3")

    # Cr√©er le bucket S3
    try:
        response = s3_client.create_bucket(
            Bucket=bucket_name,
        )
        print(f"Le bucket S3 '{bucket_name}' a √©t√© cr√©√© avec succ√®s.")
    except Exception as e:
        print(f"Erreur lors de la cr√©ation du bucket S3 : {e}")


# Nom du bucket √† cr√©er
bucket_name = "s3-transcriptionspeachtest"  # Remplacez par un nom de bucket unique

# Cr√©er le bucket S3
create_s3_bucket(bucket_name)


# <p style="text-align: left; font-size: 16px; color:#7a0f43"><span>‚ùì Quelle service AWS serait le plus indiqu√© pour r√©aliser ce traitement de transcription speech-to-text ?</span></p>

# In[ ]:




# <p style="text-align: left; font-size: 16px; color:#131fcf"><span>üñ•Ô∏è  Ecrivez le code de la fonction <code style="text-align: left; font-size: 16px; color:#131fcf">get_text_from_speech</code> permettant de r√©aliser la transcription speech-to-text avec AWS</span></p>

# <p style="text-align: left; font-size: 16px; color:#ec8f1a"><span>üìö  Voice to text using AWS Transcribe : </span> <a href="https://dev.to/botreetechnologies/voice-to-text-using-aws-transcribe-with-python-1cfc">https://dev.to/botreetechnologies/voice-to-text-using-aws-transcribe-with-python-1cfc</a></p> 

# In[37]:


import os
import time
import urllib.request
import json
import time

def generate_unique_job_name(base_name="transcription-job"):
    """
    G√©n√®re un nom de travail unique en utilisant l'horodatage.
    """
    timestamp = int(time.time())  # Utiliser l'heure actuelle en secondes
    return f"{base_name}-{timestamp}"
def get_aws_session():
    """
    Cr√©e et retourne une session AWS.
    Charge les cl√©s d'acc√®s depuis un fichier .env pour une configuration s√©curis√©e.
    """
    return boto3.Session(
        aws_access_key_id=os.getenv("ACCESS_KEY"),
        aws_secret_access_key=os.getenv("SECRET_KEY"),
        region_name="us-east-1"  # Vous pouvez changer la r√©gion si n√©cessaire
    )
def get_text_from_speech(filename, aws_service,job_name,bucket_name):
    """
    Convertit de la parole en texte en utilisant AWS Transcribe.

    Cette fonction t√©l√©verse un fichier audio sp√©cifi√© dans un seau S3, d√©marre un travail de transcription avec AWS Transcribe,
    attend que le travail soit termin√©, et r√©cup√®re le texte transcrit.

    Param√®tres :
    - filename (str) : Chemin local vers le fichier audio √† transcrire.
    - aws_service (object) : Client AWS Transcribe configur√©.
    - job_name (str) : Nom unique pour le travail de transcription.
    - bucket_name (str) : Nom du seau S3 o√π le fichier audio est stock√©.

    Retourne :
    - str : Le texte transcrit du fichier audio.

    Pr√©requis :
    - Le fichier audio doit d√©j√† √™tre t√©l√©vers√© dans le seau S3 sp√©cifi√©.
    """
    aws_session = get_aws_session()
    s3_client = aws_session.client('s3')

    # T√©l√©verser le fichier audio dans S3
    try:
        s3_client.upload_file(filename, bucket_name, os.path.basename(filename))
        print(f"Fichier {filename} t√©l√©vers√© avec succ√®s dans le seau {bucket_name}.")
    except Exception as e:
        print(f"Erreur lors du t√©l√©versement du fichier : {e}")
        return None

    # URI du fichier audio dans S3
    media_uri = f"s3://{bucket_name}/{os.path.basename(filename)}"

    # Cr√©er un client AWS Transcribe
    transcribe_client = aws_session.client('transcribe', region_name='us-east-1')

    # G√©n√©rer un nom unique pour la t√¢che de transcription
    unique_job_name = generate_unique_job_name(job_name)

    # D√©marrer la t√¢che de transcription
    try:
        transcribe_client.start_transcription_job(
            TranscriptionJobName=unique_job_name,
            LanguageCode="fr-FR",  # Langue de la transcription (ici, fran√ßais)
            Media={'MediaFileUri': media_uri},
            OutputBucketName=bucket_name  # Le bucket o√π stocker la sortie transcrite
        )
        print(f"T√¢che de transcription '{unique_job_name}' lanc√©e pour {filename}.")
    except Exception as e:
        print(f"Erreur lors du d√©marrage de la transcription : {e}")
        return None

    # Attendre la fin de la transcription
    while True:
        status = transcribe_client.get_transcription_job(TranscriptionJobName=unique_job_name)
        job_status = status['TranscriptionJob']['TranscriptionJobStatus']
        
        if job_status in ['COMPLETED', 'FAILED']:
            break
        else:
            print("En attente de la fin de la transcription...")
            time.sleep(30)  # Attendre 30 secondes avant de v√©rifier √† nouveau

    if job_status == 'COMPLETED':
        # R√©cup√©rer le r√©sultat de la transcription
        transcription_url = status['TranscriptionJob']['Transcript']['TranscriptFileUri']
        print(f"Transcription termin√©e. R√©sultat disponible ici : {transcription_url}")

        # T√©l√©charger le fichier JSON de transcription
        try:
            response = urllib.request.urlopen(transcription_url)
            transcript_data = json.loads(response.read())
            transcript_text = transcript_data['results']['transcripts'][0]['transcript']
            return transcript_text
        except Exception as e:
            print(f"Erreur lors du t√©l√©chargement ou de l'analyse du fichier de transcription : {e}")
            return None
    else:
        print(f"Le travail de transcription a √©chou√©. Statut : {job_status}")
        return None


# <p style="text-align: left; font-size: 16px; color:#131fcf"><span>üñ•Ô∏è  Ecrivez le code permettant de tester la fonction  <strong>get_text_from_speech</strong>. Pour ce faire : <ul style="text-align: left; font-size: 16px; color:#131fcf">
#     <li>Uploader la vid√©o de test sur le bucket de test pr√©alablement cr√©√©</li>
#     <li>Instancier le service AWS appropri√© pour ce traitement </li>
#     <li>Appelez la fonction <code style="text-align: left; font-size: 16px; color:#131fcf">get_text_from_speech</code> avec ce service comme argument afin de recueillir le texte recueilli</li>
#     </ul> </span></p>

# In[38]:


import os
import boto3
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Fonction pour cr√©er la session AWS
def get_aws_session():
    """
    Cr√©e et retourne une session AWS.
    Charge les cl√©s d'acc√®s depuis un fichier .env pour une configuration s√©curis√©e.
    """
    return boto3.Session(
        aws_access_key_id=os.getenv("ACCESS_KEY"),
        aws_secret_access_key=os.getenv("SECRET_KEY"),
        region_name="us-east-1"  # Vous pouvez changer la r√©gion si n√©cessaire
    )

# Fonction pour extraire les expressions cl√©s
def extract_keyphrases(text, aws_service):
    """
    Extrait les expressions cl√©s d'un texte et retourne les 10 expressions les plus pertinentes comme hashtags.
    """
    try:
        # Appeler AWS Comprehend pour extraire les expressions cl√©s
        response = aws_service.detect_key_phrases(Text=text, LanguageCode="fr")
        
        # R√©cup√©rer les expressions cl√©s tri√©es par score de pertinence
        key_phrases = [phrase['Text'] for phrase in sorted(response['KeyPhrases'], key=lambda x: x['Score'], reverse=True)]
        
        # Retourner les 10 expressions cl√©s les plus pertinentes
        return key_phrases[:10]
    
    except Exception as e:
        print(f"Erreur lors de l'extraction des expressions cl√©s : {e}")
        return []

# Cr√©er la session AWS
aws_session = get_aws_session()

# Cr√©er le client AWS Comprehend
aws_comprehend_client = aws_session.client('comprehend', region_name='us-east-1')


# Appeler la fonction pour extraire les expressions cl√©s
#key_phrases = extract_keyphrases(texte_nettoye, aws_comprehend_client)

# Afficher les r√©sultats
print("Les 10 expressions cl√©s extraites sont :")
# for phrase in key_phrases:
#     print(f"#{phrase}")


# <h4 style="text-align: left; color:#20a08d; font-size: 25px"><span><strong> Production de hashtags d'une s√©quence vid√©o
# </strong></span></h4>

# La production de hashtag sur une s√©quence vid√©o se base sur le texte extrait de la vid√©o apr√®s l'√©tape de speech-to-text, qui sera utilis√© pour en extraire des mots-cl√©s (keyphrases). Au pr√©alable, le texte extrait devra √™tre nettoy√© pour y enlever quelques √©l√©ments inutiles. C'est la fonction de la fonction `clean_text`

# In[65]:


import nltk
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer

# T√©l√©charger les stopwords si ce n'est pas d√©j√† fait
nltk.download('stopwords')

def clean_text(raw_text):
    """
    Nettoie un texte en retirant les mots vides et en normalisant les mots en minuscules.

    Cette fonction prend un texte brut en entr√©e, tokenise le texte pour s√©parer les mots,
    convertit les mots en minuscules, et retire les mots vides (stop words) en fran√ßais. Les mots vides
    suppl√©mentaires peuvent √™tre ajout√©s √† la liste. Le texte r√©sultant contient uniquement les mots significatifs
    en minuscules.

    Param√®tres :
    - raw_text (str) : Le texte brut √† nettoyer.

    Retourne :
    - str : Le texte nettoy√©, sans mots vides et en minuscules.

    Exemple d'utilisation :
    >>> texte_brut = "Ceci est un exemple de texte √† nettoyer."
    >>> clean_text(texte_brut)
    'exemple texte nettoyer'
    """
    
    # Tokenizer pour s√©parer le texte en mots
    tokenizer = RegexpTokenizer(r'\w+')
    words = tokenizer.tokenize(raw_text.lower())  # Mettre en minuscules et tokeniser

    # Charger les stopwords en fran√ßais
    stop_words = set(stopwords.words('french'))

    # Filtrer les mots pour retirer les mots vides et les mots trop courts (1-2 lettres)
    cleaned_text = [word for word in words if word not in stop_words and len(word) > 2]

    # Retourner le texte nettoy√© sous forme d'une cha√Æne de caract√®res
    return ' '.join(cleaned_text)


# <p style="text-align: left; font-size: 16px; color:#131fcf"><span>üñ•Ô∏è  Appelez la fonction <code style="text-align: left; font-size: 16px; color:#131fcf">clean_text</code> le texte extrait afin de recueillir un texte nettoy√©</span></p>

# In[66]:


# print(transcript_text)
# texte_nettoye = clean_text(transcript_text)
# print(texte_nettoye)


# <p style="text-align: left; font-size: 16px; color:#7a0f43"><span>‚ùì Quelle service AWS serait le plus indiqu√© pour r√©aliser ce traitement d'extraction des "key phrases" ?</span></p>

# In[67]:



# <p style="text-align: left; font-size: 16px; color:#131fcf"><span>üñ•Ô∏è  Ecrivez le code de la fonction <code style="text-align: left; font-size: 16px; color:#131fcf">extract_keyphrases</code> permettant d'extraire les mots cl√©s d'un texte en entr√©e. Ne retenez que les 10 mots-cl√©s d√©tect√©s avec le plus de confiance</span></p>

# In[79]:


import boto3

def extract_keyphrases(text, aws_service):
    """
    Extrait les expressions cl√©s d'un texte et retourne les 10 expressions les plus pertinentes comme hashtags.

    Cette fonction utilise un service AWS, tel que Amazon Comprehend, pour d√©tecter les expressions cl√©s dans
    un texte donn√©. Elle trie ces expressions par leur score de pertinence fourni par AWS et retourne les 10
    expressions cl√©s les plus pertinentes sous forme de hashtags.

    Param√®tres :
    - text (str) : Le texte duquel extraire les expressions cl√©s.
    - aws_service (object) : Un objet de service AWS configur√© pour d√©tecter les expressions cl√©s.

    Retourne :
    - list[str] : Une liste des 10 hashtags les plus pertinents bas√©s sur les expressions cl√©s du texte.

    Exemple d'utilisation :
    >>> aws_comprehend_client = boto3.client('comprehend', region_name='us-east-1')
    >>> extract_keyphrases("Ceci est un exemple de texte.", aws_comprehend_client)
    ['#exemple', '#texte']
    """

    # Utilisation du service AWS Comprehend pour d√©tecter les expressions cl√©s
    try:
        # Appel √† la m√©thode detect_key_phrases pour extraire les expressions cl√©s
        response = aws_service.detect_key_phrases(Text=text, LanguageCode='fr')  # 'fr' pour fran√ßais

        # Extraire les expressions cl√©s avec leur score de confiance
        key_phrases = response['KeyPhrases']

        # Trier les expressions cl√©s par score de pertinence (du plus √©lev√© au plus bas)
        sorted_key_phrases = sorted(key_phrases, key=lambda x: x['Score'], reverse=True)

        # Utiliser un ensemble pour suivre les expressions d√©j√† ajout√©es
        added_phrases = set()

        # Cr√©er la liste des hashtags sans doublons
        top_10_key_phrases = []
        for phrase in sorted_key_phrases:
            # Convertir chaque expression en hashtag
            hashtag = f"{phrase['Text'].replace(' ', '').lower()}"
            # Ajouter le hashtag si ce n'est pas d√©j√† dans la liste
            if hashtag not in added_phrases:
                top_10_key_phrases.append(hashtag)
                added_phrases.add(hashtag)

            # Si on a d√©j√† 10 hashtags, on arr√™te d'ajouter
            if len(top_10_key_phrases) == 10:
                break

        return top_10_key_phrases
    
    except Exception as e:
        print(f"Erreur lors de l'extraction des expressions cl√©s : {e}")
        return []


# <p style="text-align: left; font-size: 16px; color:#131fcf"><span>üñ•Ô∏è  Ecrivez le code permettant de tester la fonction  <strong>extract_keyphrases</strong>. Pour ce faire : <ul style="text-align: left; font-size: 16px; color:#131fcf">
#     <li>Instancier le service AWS appropri√© pour ce traitement </li>
#     <li>Appelez la fonction <code style="text-align: left; font-size: 16px; color:#131fcf">extract_keyphrases</code> avec ce service comme argument afin de recueillir la liste des mots-cl√©s</li>
#     </ul> </span></p>

# In[80]:


import os
import boto3
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Fonction pour cr√©er la session AWS
def get_aws_session():
    """
    Cr√©e et retourne une session AWS.
    Charge les cl√©s d'acc√®s depuis un fichier .env pour une configuration s√©curis√©e.
    """
    return boto3.Session(
        aws_access_key_id=os.getenv("ACCESS_KEY"),
        aws_secret_access_key=os.getenv("SECRET_KEY"),
        region_name="us-east-1"  # Vous pouvez changer la r√©gion si n√©cessaire
    )

# aws_session = get_aws_session()
# print(transcript_text)
# texte_nettoye = clean_text(transcript_text)
# print(texte_nettoye)
# # Cr√©er le client AWS Comprehend
# aws_comprehend_client = aws_session.client('comprehend', region_name='us-east-1')

# # Appeler la fonction pour extraire les expressions cl√©s
# key_phrases = extract_keyphrases(texte_nettoye, aws_comprehend_client)

# # Afficher les r√©sultats
# print("Les 10 expressions cl√©s extraites sont :")
# for phrase in key_phrases:
#     print(f"#{phrase}")


# <h4 style="text-align: left; color:#20a08d; font-size: 25px"><span><strong> Production de hashtags d'une image
# </strong></span></h4>

# La production de hashtags sur une image se base sur la d√©tection des objets et des c√©l√©brit√©s pr√©sents dans l'image.

# <h4 style="text-align: left; color:#20a08d; font-size: 20px"><span><strong> D√©tection d'objets sur une image
# </strong></span></h4>

# <p style="text-align: left; font-size: 16px; color:#131fcf"><span>üñ•Ô∏è  Ecrivez le code de la fonction <code style="text-align: left; font-size: 16px; color:#131fcf">detect_objects</code> permettant de d√©tecter les objets pr√©sents sur une image donn√©e en entr√©e de la fonction. Ne retenez que les 10 objets d√©tect√©s avec le plus de confiance.</span></p>

# In[81]:


def detect_objects(image_path, aws_service):
    """
    D√©tecte les objets dans une image en utilisant Amazon Rekognition.

    Cette fonction ouvre une image depuis un chemin sp√©cifi√©, utilise un service AWS (Amazon Rekognition) pour
    d√©tecter les objets pr√©sents dans l'image avec une confiance minimale de 50%, et retourne les noms des 10
    objets les plus pertinents d√©tect√©s.

    Param√®tres :
    - image_path (str) : Le chemin vers l'image √† analyser.
    - aws_service (object) : Un client AWS Rekognition configur√©.

    Retourne :
    - list[str] : Une liste contenant les noms des 10 premiers objets d√©tect√©s dans l'image.

    Exemple d'utilisation :
    >>> aws_rekognition_client = boto3.client('rekognition', region_name='us-east-1')
    >>> detect_objects("/chemin/vers/image.jpg", aws_rekognition_client)
    ['Voiture', 'Arbre', 'Personne']
    """
    try:
        # Ouvrir l'image et la lire en binaire
        with open(image_path, 'rb') as image_file:
            image_bytes = image_file.read()

        # Appel √† Amazon Rekognition pour d√©tecter les objets
        response = aws_service.detect_labels(
            Image={'Bytes': image_bytes},
            MaxLabels=10,  # Limite √† 10 objets d√©tect√©s
            MinConfidence=50  # Confiance minimale de 50%
        )
        
        # Extraire les objets d√©tect√©s avec leurs scores de confiance
        labels = response['Labels']
        
        # Trier les labels par score de confiance (du plus √©lev√© au plus bas)
        sorted_labels = sorted(labels, key=lambda x: x['Confidence'], reverse=True)

        # Extraire les 10 objets avec les plus hauts scores de confiance
        top_objects = [label['Name'] for label in sorted_labels[:10]]

        return top_objects
    
    except Exception as e:
        print(f"Erreur lors de la d√©tection des objets : {e}")
        return []


# <p style="text-align: left; font-size: 16px; color:#131fcf"><span>üñ•Ô∏è  Ecrivez le code permettant de tester la fonction  <strong>detect_objects</strong>. Pour ce faire : <ul style="text-align: left; font-size: 16px; color:#131fcf">
#     <li>Instancier le service AWS appropri√© pour ce traitement </li>
#     <li>Appelez la fonction <code style="text-align: left; font-size: 16px; color:#131fcf">detect_objects</code> avec ce service comme argument afin de recueillir la liste des objets pr√©sents sur cette image de test</li>
#     </ul> </span></p>

# In[83]:


image_path = './assets/no-violence4.png'
aws_session = get_aws_session()
# Instancier le client AWS Rekognition
aws_rekognition_client = aws_session.client('rekognition', region_name='us-east-1')

# Appeler la fonction pour d√©tecter les objets dans l'image
objects = detect_objects(image_path, aws_rekognition_client)

# Afficher les 10 objets d√©tect√©s
print("Les objets d√©tect√©s sont :")
for obj in objects:
    print(obj)


# <h4 style="text-align: left; color:#20a08d; font-size: 20px"><span><strong> D√©tection des c√©l√©brit√©s sur une image
# </strong></span></h4>

# <p style="text-align: left; font-size: 16px; color:#131fcf"><span>üñ•Ô∏è  Ecrivez le code de la fonction <code style="text-align: left; font-size: 16px; color:#131fcf">detect_celebrities</code> permettant de d√©tecter les c√©l√©brit√©s pr√©sents sur une image donn√©e en entr√©e de la fonction.</span></p>

# In[84]:


def detect_celebrities(image_path, aws_service):
    """
    Identifie les c√©l√©brit√©s dans une image en utilisant le service Amazon Rekognition.

    Cette fonction ouvre une image depuis un chemin donn√© et utilise le service AWS Rekognition pour reconna√Ætre les
    c√©l√©brit√©s pr√©sentes dans l'image. Elle retourne une liste contenant les noms des c√©l√©brit√©s identifi√©es, limit√©e
    aux 10 premiers r√©sultats pour simplifier l'output.

    Param√®tres :
    - image_path (str) : Le chemin vers l'image dans laquelle d√©tecter les c√©l√©brit√©s.
    - aws_service (object) : Un client AWS Rekognition configur√©.

    Retourne :
    - list[str] : Une liste des noms des c√©l√©brit√©s identifi√©es dans l'image, jusqu'√† un maximum de 10.

    Exemple d'utilisation :
    >>> aws_rekognition_client = boto3.client('rekognition', region_name='us-east-1')
    >>> detect_celebrities("/chemin/vers/limage.jpg", aws_rekognition_client)
    ['Leonardo DiCaprio', 'Kate Winslet']
    """
    try:
        # Ouvrir l'image et la lire en binaire
        with open(image_path, 'rb') as image_file:
            image_bytes = image_file.read()

        # Appel √† Amazon Rekognition pour d√©tecter les c√©l√©brit√©s
        response = aws_service.recognize_celebrities(
            Image={'Bytes': image_bytes}
        )

        # Extraire les c√©l√©brit√©s d√©tect√©es
        celebrities = response['CelebrityFaces']

        # Extraire les noms des c√©l√©brit√©s d√©tect√©es
        celebrity_names = [celebrity['Name'] for celebrity in celebrities]

        # Limiter √† 10 c√©l√©brit√©s
        top_celebrity_names = celebrity_names[:10]

        return top_celebrity_names

    except Exception as e:
        print(f"Erreur lors de la d√©tection des c√©l√©brit√©s : {e}")
        return []


# <p style="text-align: left; font-size: 16px; color:#131fcf"><span>üñ•Ô∏è  Ecrivez le code permettant de tester la fonction  <strong>detect_celebrities</strong>. Pour ce faire : <ul style="text-align: left; font-size: 16px; color:#131fcf">
#     <li>Instancier le service AWS appropri√© pour ce traitement </li>
#     <li>Appelez la fonction <code style="text-align: left; font-size: 16px; color:#131fcf">detect_celebrities</code> avec ce service comme argument afin de recueillir la liste des c√©l√©brit√©s pr√©sentes sur chacune des images de test</li>
#     </ul> </span></p>

# 
# def test_detect_celebrities():
#     aws_session = get_aws_session()
#     # Instancier le client AWS Rekognition
#     aws_rekognition_client = aws_session.client('rekognition', region_name='us-east-1')
# 
#     # Liste des chemins vers les images de test
#     image_paths = [
#         './assets/selfie_with_mariah-carey.png',  # Remplacer par le chemin r√©el de l'image 1
#         './assets/selfie_with_johnny-depp.png',  # Remplacer par le chemin r√©el de l'image 2
#         './assets/selfie_with_kanye-west.png'   # Remplacer par le chemin r√©el de l'image 3
#     ]
# 
#     # Tester la d√©tection des c√©l√©brit√©s pour chaque image
#     for image_path in image_paths:
#         print(f"\nD√©tection des c√©l√©brit√©s pour l'image : {image_path}")
#         celebrities = detect_celebrities(image_path, aws_rekognition_client)
#         
#         if celebrities:
#             print("C√©l√©brit√©s d√©tect√©es :")
#             for celeb in celebrities:
#                 print(f"- {celeb}")
#         else:
#             print("Aucune c√©l√©brit√© d√©tect√©e.")
# 
# # Appeler la fonction de test
# test_detect_celebrities()

# <h4 style="text-align: left; color:#20a08d; font-size: 20px"><span><strong> Reconnaissance d'√©motion faciale sur une image
# </strong></span></h4>

# <span style="color:#131fcf">üñ•Ô∏è Codez la fonction `detect_emotions` qui doit :
# 
# <ul style="color:#131fcf">
# <li>Prendre en entr√©e :
#   <ul>
#     <li>Le chemin de l'image √† analyser</li>
#     <li>Le client AWS Rekognition configur√©</li>
#   </ul>
# </li>
# 
# <li>Analyser l'image :
#   <ul>
#     <li>Ouvrir l'image en mode binaire</li>
#     <li>Utiliser Rekognition avec <strong>detect_faces</strong></li>
#     <li>Demander tous les attributs (Attributes=['ALL'])</li>
#   </ul>
# </li>
# 
# <li>Pour chaque visage d√©tect√©, afficher :
#   <ul>
#     <li>Le genre avec son niveau de confiance</li>
#     <li>L'√¢ge estim√© (range min-max)</li>
#     <li>Les 3 √©motions principales avec leur niveau de confiance</li>
#   </ul>
# </li>
# 
# <li>Retourner la liste compl√®te des informations des visages d√©tect√©s</li>
# 
# <li>Exemple de sortie console attendue :
# <code style="color:#131fcf">
# [INFO] Visage d√©tect√©:
#   - Genre: Male (confiance: 99.9%)
#   - √Çge estim√©: 20-30 ans
#   - √âmotions principales:
#     * HAPPY: 95.5%
#     * CALM: 4.5%
# ---
# </code>
# </li>
# </ul>
# </span>

# In[91]:


def detect_emotions(image_path, aws_service):
    """
    D√©tecte les √©motions sur les visages pr√©sents dans une image en utilisant Amazon Rekognition.
    
    Cette fonction analyse une image pour d√©tecter les visages et leurs √©motions associ√©es.
    Pour chaque visage, elle retourne les √©motions d√©tect√©es avec leur niveau de confiance.
    
    Param√®tres :
    - image_path (str) : Chemin vers l'image √† analyser
    - aws_service (boto3.client) : Client AWS Rekognition configur√©
    
    Retourne :
    - list[dict] : Liste des visages d√©tect√©s avec leurs √©motions
                  Format: [
                      {
                          'BoundingBox': dict,
                          'Emotions': [
                              {
                                  'Type': str,  # HAPPY, SAD, ANGRY, CONFUSED, etc.
                                  'Confidence': float
                              },
                              ...
                          ],
                          'AgeRange': {'Low': int, 'High': int},
                          'Gender': {'Value': str, 'Confidence': float}
                      },
                      ...
                  ]
    
    Exemple :
    >>> rekognition = boto3.client('rekognition')
    >>> emotions = detect_emotions("./photo.jpg", rekognition)
    >>> for face in emotions:
    ...     print(f"√âmotions d√©tect√©es : {face['Emotions']}")
    """
    try:
        # Ouvrir l'image et la lire en binaire
        with open(image_path, 'rb') as image_file:
            image_bytes = image_file.read()

        # Appel √† Amazon Rekognition pour d√©tecter les visages avec tous les attributs
        response = aws_service.detect_faces(
            Image={'Bytes': image_bytes},
            Attributes=['ALL']  # Demander tous les attributs, y compris les √©motions
        )

        # Initialiser la liste pour stocker les informations des visages d√©tect√©s
        faces_info = []

        # Parcourir les visages d√©tect√©s dans la r√©ponse
        for face_detail in response['FaceDetails']:
            # Cr√©er un dictionnaire pour chaque visage d√©tect√©
            face_data = {}

            # R√©cup√©rer le genre et son niveau de confiance
            face_data['Gender'] = {
                'Value': face_detail['Gender']['Value'],
                'Confidence': face_detail['Gender']['Confidence']
            }

            # R√©cup√©rer l'√¢ge estim√© (plage min-max)
            face_data['AgeRange'] = face_detail['AgeRange']

            # R√©cup√©rer les √©motions avec leur niveau de confiance
            emotions = face_detail['Emotions']
            face_data['Emotions'] = sorted(emotions, key=lambda x: x['Confidence'], reverse=True)[:3]  # Prendre les 3 √©motions principales

            # Ajouter les informations du visage √† la liste
            faces_info.append(face_data)

            # Affichage des informations sur le visage d√©tect√©
            print("[INFO] Visage d√©tect√©:")
            print(f"  - Genre: {face_data['Gender']['Value']} (confiance: {face_data['Gender']['Confidence']}%)")
            print(f"  - √Çge estim√©: {face_data['AgeRange']['Low']}-{face_data['AgeRange']['High']} ans")
            print("  - √âmotions principales:")
            for emotion in face_data['Emotions']:
                print(f"    * {emotion['Type']}: {emotion['Confidence']:.2f}%")
            print("---")

        return faces_info

    except Exception as e:
        print(f"Erreur lors de la d√©tection des √©motions : {e}")
        return []


# <span style="color:#131fcf">üñ•Ô∏è Codez la fonction `summarize_emotions` qui doit :
# 
# <ul style="color:#131fcf">
# <li>Prendre en entr√©e une liste de visages d√©tect√©s dans une image comme fourni par la fonction <code>detect_emotions</code></li>
# <li>Exemple d'entr√©e :
# <code style="color:#131fcf">
# [{
#     'Gender': {'Value': 'Male', 'Confidence': 99.9},
#     'AgeRange': {'Low': 20, 'High': 30},
#     'Emotions': [
#         {'Type': 'HAPPY', 'Confidence': 95.5},
#         {'Type': 'CALM', 'Confidence': 4.5}
#     ]
# }]
# </code>
# <li>Pour chaque visage, analyser :
#   <ul>
#     <li>Le genre (Homme/Femme)</li>
#     <li>L'√¢ge (calcul de la moyenne du range)</li>
#     <li>Les √©motions avec une confiance > 50%</li>
#   </ul>
# </li>
# 
# <li>Retourner un dictionnaire avec :
#   <ul>
#     <li>Nombre total de visages</li>
#     <li>√âmotion dominante (celle avec la plus haute confiance moyenne)</li>
#     <li>Statistiques des √©motions (comptage et confiance moyenne)</li>
#     <li>Statistiques d'√¢ge (min, max, moyenne)</li>
#     <li>Distribution des genres</li>
#   </ul>
# </li>
# </li>
# </ul>
# </span>

# In[92]:


def summarize_emotions(faces_info):
    """
    R√©sume les √©motions d√©tect√©es sur tous les visages d'une image.
    
    Cette fonction agr√®ge les √©motions de tous les visages et calcule les √©motions
    dominantes dans l'image.
    
    Param√®tres :
    - faces_info (list[dict]) : Liste des informations des visages d√©tect√©s
    
    Retourne :
    - dict : R√©sum√© des √©motions dominantes et statistiques
    
    Exemple :
    >>> emotions = detect_emotions("./group_photo.jpg", rekognition)
    >>> summary = summarize_emotions(emotions)
    >>> print(f"√âmotion dominante : {summary['dominant_emotion']}")
    """
    total_faces = len(faces_info)
    emotion_stats = {}
    age_stats = {'min': float('inf'), 'max': float('-inf'), 'total_age': 0, 'count': 0}
    gender_stats = {'Male': 0, 'Female': 0}
    dominant_emotion = {'Type': None, 'Confidence': 0}
    
    # Parcourir les visages d√©tect√©s
    for face in faces_info:
        # Analyser le genre
        gender = face['Gender']['Value']
        gender_stats[gender] += 1
        
        # Analyser l'√¢ge
        age_range = face['AgeRange']
        avg_age = (age_range['Low'] + age_range['High']) / 2
        age_stats['total_age'] += avg_age
        age_stats['count'] += 1
        age_stats['min'] = min(age_stats['min'], avg_age)
        age_stats['max'] = max(age_stats['max'], avg_age)
        
        # Analyser les √©motions avec une confiance > 50%
        for emotion in face['Emotions']:
            if emotion['Confidence'] > 50:
                # Mettre √† jour les statistiques des √©motions
                emotion_type = emotion['Type']
                if emotion_type not in emotion_stats:
                    emotion_stats[emotion_type] = {'count': 0, 'total_confidence': 0}
                
                emotion_stats[emotion_type]['count'] += 1
                emotion_stats[emotion_type]['total_confidence'] += emotion['Confidence']
                
                # Mettre √† jour l'√©motion dominante
                if emotion['Confidence'] > dominant_emotion['Confidence']:
                    dominant_emotion['Type'] = emotion_type
                    dominant_emotion['Confidence'] = emotion['Confidence']
    
    # Calculer les moyennes des √©motions
    for emotion_type, stats in emotion_stats.items():
        stats['average_confidence'] = stats['total_confidence'] / stats['count']
    
    # Calculer la moyenne des √¢ges
    avg_age = age_stats['total_age'] / age_stats['count'] if age_stats['count'] > 0 else 0

    # R√©sum√© final des r√©sultats
    summary = {
        'total_faces': total_faces,
        'dominant_emotion': dominant_emotion['Type'],
        'dominant_emotion_confidence': dominant_emotion['Confidence'],
        'emotion_stats': emotion_stats,
        'age_stats': {
            'min_age': age_stats['min'],
            'max_age': age_stats['max'],
            'average_age': avg_age
        },
        'gender_stats': gender_stats
    }

    return summary


# <ul style="color:#131fcf">
# <li>Testez la d√©tection et l'analyse d'√©motions :
#   <ul>
#     <li>Sur chacune des 4 images de groupe</li>
#     <li>Comparez les r√©sultats entre elles</li>
#   </ul>
# </li>
# <li>Pour chaque image :
#   <ul>
#     <li>Afficher les d√©tails de chaque visage d√©tect√©</li>
#     <li>G√©n√©rer le r√©sum√© des statistiques</li>
#     <li>Noter les diff√©rences d'√©motions dominantes</li>
#   </ul>
# </li>
# </li>
# </ul>
# </span>

# In[93]:


# D√©finir les chemins des images de test
aws_session = get_aws_session()
aws_rekognition_client = aws_session.client('rekognition', region_name='us-east-1')
TEST_IMAGE_FILE_1 = "./assets/group_selfie_1.jpg"    # Premier selfie de groupe
TEST_IMAGE_FILE_2 = "./assets/group_selfie_2.jpg"    # Deuxi√®me selfie de groupe
TEST_IMAGE_FILE_3 = "./assets/group_selfie_3.jpg"    # Troisi√®me selfie de groupe
TEST_IMAGE_FILE_4 = "./assets/group_selfie_4.jpg"    # Quatri√®me selfie de groupe
def print_face_details(faces_info):
    """
    Affiche les d√©tails des visages d√©tect√©s (genre, √¢ge, √©motions).
    """
    for idx, face in enumerate(faces_info):
        print(f"[INFO] Visage {idx + 1} d√©tect√©:")
        print(f"  - Genre: {face['Gender']['Value']} (confiance: {face['Gender']['Confidence']:.1f}%)")
        print(f"  - √Çge estim√©: {face['AgeRange']['Low']} - {face['AgeRange']['High']} ans")
        print(f"  - √âmotions principales:")
        for emotion in face['Emotions']:
            print(f"    * {emotion['Type']}: {emotion['Confidence']:.1f}%")


def analyze_image(image_path):
    """
    Analyse une image et affiche les d√©tails et statistiques des √©motions.
    """
    # D√©tecter les √©motions dans l'image
    faces_info = detect_emotions(image_path, aws_rekognition_client)
    
    # Afficher les d√©tails de chaque visage d√©tect√©
    print(f"Analyser l'image : {image_path}")
    print_face_details(faces_info)
    
    # R√©sumer les √©motions d√©tect√©es
    summary = summarize_emotions(faces_info)
    
    # Afficher le r√©sum√© des statistiques
    print(f"\n[INFO] R√©sum√© des √©motions :")
    print(f"  - Nombre total de visages : {summary['total_faces']}")
    print(f"  - √âmotion dominante : {summary['dominant_emotion']} (confiance: {summary['dominant_emotion_confidence']:.1f}%)")
    print(f"  - Statistiques des √©motions :")
    for emotion, stats in summary['emotion_stats'].items():
        print(f"    * {emotion}: {stats['count']} occurrences, confiance moyenne: {stats['average_confidence']:.1f}%")
    
    print(f"  - Statistiques d'√¢ge :")
    print(f"    * √Çge min : {summary['age_stats']['min_age']} ans")
    print(f"    * √Çge max : {summary['age_stats']['max_age']} ans")
    print(f"    * √Çge moyen : {summary['age_stats']['average_age']:.1f} ans")
    
    print(f"  - Distribution des genres :")
    print(f"    * Hommes : {summary['gender_stats']['Male']}")
    print(f"    * Femmes : {summary['gender_stats']['Female']}")
    print("\n" + "-" * 50)


# Analyser les images de test
analyze_image(TEST_IMAGE_FILE_1)
analyze_image(TEST_IMAGE_FILE_2)
analyze_image(TEST_IMAGE_FILE_3)
analyze_image(TEST_IMAGE_FILE_4)


# In[ ]:





# <h4 style="text-align: left; color:#20a08d; font-size: 20px"><span><strong> Fonction de traitement finale
# </strong></span></h4>

# Il est maintenant temps de d√©velopper la fonction de traitement finale `process_media` qui se basera sur l'ensemble des fonctions d√©velopp√©es pr√©c√©demment.

# <p style="text-align: left; font-size: 16px; color:#131fcf"><span>üñ•Ô∏è  Ecrivez le code de la fonction <code style="text-align: left; font-size: 16px; color:#131fcf">process_media</code> permettant de r√©aliser l'ensemble des traitements : <ul style="text-align: left; font-size: 16px; color:#131fcf">
#     <li>D√©terminer le type de m√©dia (vid√©o ou image)</li>
#     <li>Si le m√©dia est une image : </li>
#     <ul style="text-align: left; font-size: 16px; color:#131fcf">
#         <li>Mod√©rer l'image</li>
#         <li>Si aucun contenu choquant n'est d√©tect√©,  d√©tecter les objets, l'√©motion dominante des visages et les c√©l√©brit√©s pr√©sents sur l'image qui serviront de mot-cl√©s pour produire les hashtags</li>
#         <li>Si du contenu choquant est trouv√©, retourner <strong>None</strong></li>
#     </ul>
#     <li>Si le m√©dia est une vid√©o : </li>
#     <ul style="text-align: left; font-size: 16px; color:#131fcf">
#         <li>Extraire la premi√®re image de la vid√©o</li>
#         <li>Sauvegarder cette image comme fichier temporaire</li>
#         <li>Mod√©rer cette premi√®re image</li>
#         <li>Si aucun contenu choquant n'est d√©tect√© sur cette image,  convertir la voix pr√©sente sur la vid√©o en texte</li>
#         <li>Extraire les mots-cl√©s du texte extrait</li>
#         <li>Si du contenu choquant est trouv√©, retourner <strong>None</strong></li>
#     </ul>
#     <li>La sortie de cette fonction devra √™tre un dictionnaire et avoir ce format : <strong>{subtitles : "abcdefgijklm", hashtags:["hastag1", "hastag1", ...]}</strong> pour une vid√©o et <strong>{hashtags:["hastag1", "hastag1", ...]} pour une image</strong> </li>
# </ul></span></p>

# In[54]:


import os
import time
import cv2
import boto3
import tempfile
from matplotlib import pyplot as plt

def process_media(media_file, rekognition, transcribe, comprehend, bucket_name):
    """
    Traite un fichier multim√©dia (image ou vid√©o) pour mod√©rer le contenu, d√©tecter des objets/c√©l√©brit√©s,
    transcrire le discours et extraire des expressions cl√©s.

    Selon le type de fichier, cette fonction applique une cha√Æne de traitement appropri√©e en utilisant diff√©rents
    services AWS. Pour les images, elle mod√®re le contenu, d√©tecte des objets, √©motions faciales et des c√©l√©brit√©s. Pour les vid√©os,
    elle extrait une image, mod√®re le contenu, t√©l√©verse la vid√©o sur S3, transcrit le discours en texte, nettoie le texte,
    et extrait des expressions cl√©s.

    Param√®tres :
    - media_file (str) : Chemin vers le fichier multim√©dia √† traiter.
    - rekognition (object) : Client AWS Rekognition configur√©.
    - transcribe (object) : Client AWS Transcribe configur√©.
    - comprehend (object) : Client AWS Comprehend configur√©.
    - bucket_name (str) : Nom du seau S3 pour stocker les fichiers vid√©o.

    Retourne :
    - dict : Dictionnaire contenant des hashtags pour les images ou des sous-titres et hashtags pour les vid√©os.
    """
    file_extension = os.path.splitext(media_file)[1].lower()

    if file_extension in ['.jpg', '.jpeg', '.png']:
        # Si c'est une image, mod√©rer l'image
        if moderate_image(media_file, rekognition) is not None:
            return None  # Contenu choquant d√©tect√©
        
        # Si l'image est mod√©r√©e sans probl√®me, d√©tecter les objets, √©motions et c√©l√©brit√©s
        key_phrases = []

        # D√©tection des objets
        key_phrases.extend(detect_objects(media_file, rekognition))
        
        # D√©tection des √©motions et des visages
        faces = detect_emotions(media_file, rekognition)  # Utilisation de votre fonction detect_emotions
        for face in faces:
            for emotion in face['Emotions']:
                if emotion['Confidence'] > 50:
                    key_phrases.append(f"{emotion['Type'].lower()}")
        
        # D√©tection des c√©l√©brit√©s
        key_phrases.extend(detect_celebrities(media_file, rekognition))
        
        # Extraire des expressions cl√©s (hashtags)
        text = ' '.join(key_phrases)
        # Utilisation de votre fonction extract_keyphrases
        
        # Retourner les hashtags
        return {'hashtags': list(set(key_phrases))}
    elif file_extension in ['.mp4', '.mov', '.avi']:
        frame1 = extract_frame_video(media_file, 1)
        
        if frame1 is not None:
            photo_rgb = cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB)
            plt.imshow(photo_rgb)
            plt.axis('off') 
            plt.title("Premi√®re image extraite")
            plt.show()

            with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as temp_img_file:
                temp_img_path = temp_img_file.name
                cv2.imwrite(temp_img_path, frame1)  # Sauvegarder l'image en tant que fichier

            # V√©rification de contenu choquant avec le fichier enregistr√©
            if moderate_image(temp_img_path, rekognition) is not None:
                return None  # Contenu choquant d√©tect√©

            os.remove(temp_img_path)
            job_name = 'transcriptionText'
            transcript_text = get_text_from_speech(media_file, transcribe, job_name,bucket_name)
            
            texte_nettoye = clean_text(transcript_text)
            key_phrases = extract_keyphrases(texte_nettoye, comprehend)
            return {'subtitles': transcript_text , 'hashtags': list(set(key_phrases))}
            
        
        
    return None
    


# <p style="text-align: left; font-size: 16px; color:#131fcf"><span>üñ•Ô∏è  Ecrivez le code permettant de tester la fonction  <strong>process_media</strong>. Pour ce faire : <ul style="text-align: left; font-size: 16px; color:#131fcf">
#         <li>Instancier une session AWS avec vos cl√©s</li>
#     <li>Instancier les services AWS appropri√©s pour tous les traitements </li>
#     <li>Appelez la fonction <code style="text-align: left; font-size: 16px; color:#131fcf">process_media</code> sur l'image de test et la vid√©o de test afin d'en v√©rifier le bon fonctionnement </li>
#     </ul> </span></p>

# In[55]:


# aws_session = get_aws_session()
# aws_rekognition_client = aws_session.client('rekognition', region_name='us-east-1')
# aws_comprehend_client = aws_session.client('comprehend', region_name='us-east-1')
# aws_transcribe_client = aws_session.client('transcribe', region_name='us-east-1')
# TEST_VIDEO_FILE = "./assets/tuto_jeux-video.mp4"
# TEST_IMAGE_FILE = "./assets/selfie_with_johnny-depp.png"
# BUCKET_NAME = 's3-transcriptionspeachtest'

# image = process_media(TEST_IMAGE_FILE,aws_rekognition_client,aws_transcribe_client ,aws_comprehend_client,BUCKET_NAME)
# video = process_media(TEST_VIDEO_FILE,aws_rekognition_client,aws_transcribe_client ,aws_comprehend_client,BUCKET_NAME)
# print(image)
# print(video)


# <h4 style="text-align: left; color:#20a08d; font-size: 25px"><span><strong> Resources üìöüìö</strong></span></h4>
# 
# * <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/translate.html" target="_blank">Translate with Boto3</a>
# * <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html#Textract.Client.start_document_text_detection" target="_blank">Textract Documentation</a>
# * <a href="https://aws.amazon.com/textract/" target="_blank">Textract Landing</a>
